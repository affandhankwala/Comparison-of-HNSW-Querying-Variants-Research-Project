## Introduction
This repo contains research and development information on heirarchical navigable small world (HNSW) data structures as a type of approximate nearest neighbor (ANN) search method along the GloVe dataset. We specifically compared the standard HNSW data structure with two variants: fast inference for graph-based approximate nearest neighbor search (FINGER) and distance adaptive beam search (DABS) to determine the benefits and drawbacks of these variants and how well they tackle problems that traditional HNSWS structures struggle with. Below is the abstract from the research paper:

## Abstract
High dimensional data is defined as data points with a high number of features--implying that each data point is defined by a high number of attributes. While expensive, high dimensional data has found its way into crucial fields such as computer vision, natural language processing (NLP) and others. Traditional techniques are either too slow or simply unable to account for the high dimensionality of these datasets. Approximate nearest neighbor (ANN) attempts to counter this concern by estimating the nearest neighbors to any query rather than computing them exactly. Navigable small world (NSW) is a technique based on ANN consisting of nodes connected by edges but frequently terminates at local minimums. Hierarchical navigable small world (HNSW) attempts to mitigates this issue by layering the graph at different levels of abstraction but is still susceptible to premature termination. This paper identifies two HNSW variants: Fast inference for graph-based approximate nearest neighbor search (FINGER) and distance adaptive beam search (DABS) that claim to reduce querying computations and improve recall rates, respectively. These variants are tested on four GloVe datasets ranging from 25 to 200 dimensions where we measure the comparison counts and recall rates of the models when retrieving k neighbors with k values between 5 and 50. FINGER managed to match and exceed HNSW recall rates at high levels of K while calculating fewer distances. DABS demonstrated high performance at the cost of calculating more distances on the 25 dimension dataset but failed to beat the other two variants recall rates on the other datasets due to poor tuning caused by computational resource limitation. It's performance also inverted at higher values of k on the 200 dimension dataset, hinting at some potential high dimensional proficiency. However, limited by dataset availability and computational resources, this claim could not be further analyzed. These results also do not support HNSW as being well performing in high-scale high-dimensional spaces at high levels of k. Further research could attempt to analyze the topological structure of HNSW layers and research HNSW-specific graph traversal methods that incorporate layer height and sparsity in their heuristic calculation. 